{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !!! MAKE SURE TO LOGIN ON HUGGINGFACE-CLI !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo apt-get update && sudo apt-get install -y libgoogle-perftools-dev libsparsehash-dev unzip git cmake make"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install transformers datasets tqdm regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import regex as re\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import transformers\n",
    "\n",
    "OLD_LANGUAGE='en'\n",
    "OLD_TOKENIZER='mistralai/Mistral-7B-v0.1'#'meta-llama/Llama-2-7b-hf'#\n",
    "\n",
    "NEW_LANGUAGE='nl'\n",
    "NEW_TOKENIZER='GroNLP/gpt2-small-dutch'#'FremyCompany/roberta-base-nl-oscar23'#\n",
    "\n",
    "CORPUS_LIST = ['OpenSubtitles2018','NLLB']\n",
    "CORPUS_LIST_STR = \"_\".join(CORPUS_LIST)\n",
    "\n",
    "ALIGNMENT_UNIT = \"WORDS\" # \"TOKENS\" or \"WORDS\"\n",
    "MIN_COUNT_REQUIRED_FOR_CONSIDERATION = 20\n",
    "\n",
    "OLD_TOKENIZER_FRIENDLY_NAME=OLD_TOKENIZER.replace('/','--')\n",
    "NEW_TOKENIZER_FRIENDLY_NAME=NEW_TOKENIZER.replace('/','--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load tokenizers for the two models\n",
    "old_tokenizer = transformers.AutoTokenizer.from_pretrained(OLD_TOKENIZER)\n",
    "new_tokenizer = transformers.AutoTokenizer.from_pretrained(NEW_TOKENIZER)\n",
    "\n",
    "# save the vocabularies in a set for improved performance\n",
    "old_tokenizer_vocab = set(old_tokenizer.vocab.keys())\n",
    "new_tokenizer_vocab = set(new_tokenizer.vocab.keys())\n",
    "\n",
    "# determine the tokenizer settings\n",
    "OLD_TOKENIZER_1ST_PREFIX = (old_tokenizer.convert_ids_to_tokens(old_tokenizer.encode(\" a\", add_special_tokens=False)[0]).rstrip(\"a\"))\n",
    "NEW_TOKENIZER_1ST_PREFIX = (new_tokenizer.convert_ids_to_tokens(new_tokenizer.encode(\" a\", add_special_tokens=False)[0]).rstrip(\"a\"))\n",
    "OLD_TOKENIZER_2ND_PREFIX = (old_tokenizer.convert_ids_to_tokens(old_tokenizer.encode(\"aaaaaaaaaaaaaaaaaaaaaa\", add_special_tokens=False)[1]).rstrip('a'))\n",
    "NEW_TOKENIZER_2ND_PREFIX = (new_tokenizer.convert_ids_to_tokens(new_tokenizer.encode(\"aaaaaaaaaaaaaaaaaaaaaa\", add_special_tokens=False)[1]).rstrip('a'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# preprocess a parallel corpus, to transform it into a space-delimited corpus of tokens\n",
    "\n",
    "os.makedirs(f'alignments/', exist_ok=True)\n",
    "os.makedirs(f'corpora/parallel/', exist_ok=True)\n",
    "os.makedirs(f'corpora/monolingual/', exist_ok=True)\n",
    "\n",
    "if os.path.exists(f'alignments/{CORPUS_LIST_STR}.{OLD_LANGUAGE}-{NEW_LANGUAGE}.{OLD_TOKENIZER_FRIENDLY_NAME}-{NEW_TOKENIZER_FRIENDLY_NAME}-{ALIGNMENT_UNIT}.fast_align.tsv'):\n",
    "    print(f'data already aligned')\n",
    "\n",
    "else:\n",
    "\n",
    "    if not(os.path.exists(f'alignments/{CORPUS_LIST_STR}.{OLD_LANGUAGE}-{NEW_LANGUAGE}.{OLD_TOKENIZER_FRIENDLY_NAME}-{NEW_TOKENIZER_FRIENDLY_NAME}-{ALIGNMENT_UNIT}.moses')):\n",
    "\n",
    "        #\n",
    "        # step 1: download the parallel corpus (if not already downloaded)\n",
    "        #\n",
    "\n",
    "        def download_corpus_from_nlpl_eu(corpus, corpus_folder=None, corpus_version='v1'):\n",
    "            corpus_folder = corpus_folder if corpus_folder is not None else corpus\n",
    "            if os.path.exists(f'corpora/parallel/{corpus}/{corpus}.{OLD_LANGUAGE}-{NEW_LANGUAGE}.{OLD_LANGUAGE}') and os.path.exists(f'NLLB/NLLB.{OLD_LANGUAGE}-{NEW_LANGUAGE}.{NEW_LANGUAGE}'):\n",
    "                print(f'{corpus}: data already downloaded')\n",
    "            else:\n",
    "                print(f'{corpus}: downloading data...')\n",
    "                os.makedirs(f'corpora/parallel/{corpus}', exist_ok=True)\n",
    "                if not os.path.exists(f'corpora/parallel/{corpus}/{corpus}.{OLD_LANGUAGE}-{NEW_LANGUAGE}.txt.zip'):\n",
    "                    os.system(f'wget -O corpora/parallel/{corpus}/{corpus}.{OLD_LANGUAGE}-{NEW_LANGUAGE}.txt.zip https://opus.nlpl.eu/download.php?f={corpus_folder}/{corpus_version}/moses/{OLD_LANGUAGE}-{NEW_LANGUAGE}.txt.zip')\n",
    "                os.system(f'cd corpora/parallel/{corpus} && unzip {corpus}.{OLD_LANGUAGE}-{NEW_LANGUAGE}.txt.zip')\n",
    "                # check if that worked\n",
    "                if not os.path.exists(f'corpora/parallel/{corpus}/{corpus}.{OLD_LANGUAGE}-{NEW_LANGUAGE}.{OLD_LANGUAGE}') or not os.path.exists(f'corpora/parallel/{corpus}/{corpus}.{OLD_LANGUAGE}-{NEW_LANGUAGE}.{NEW_LANGUAGE}'):\n",
    "                    # check if the files are there but with corpus_folder as their prefix\n",
    "                    if os.path.exists(f'corpora/parallel/{corpus}/{corpus_folder}.{OLD_LANGUAGE}-{NEW_LANGUAGE}.{OLD_LANGUAGE}') and os.path.exists(f'corpora/parallel/{corpus}/{corpus_folder}.{OLD_LANGUAGE}-{NEW_LANGUAGE}.{NEW_LANGUAGE}'):\n",
    "                        os.system(f'mv corpora/parallel/{corpus}/{corpus_folder}.{OLD_LANGUAGE}-{NEW_LANGUAGE}.{OLD_LANGUAGE} corpora/parallel/{corpus}/{corpus}.{OLD_LANGUAGE}-{NEW_LANGUAGE}.{OLD_LANGUAGE}')\n",
    "                        os.system(f'mv corpora/parallel/{corpus}/{corpus_folder}.{OLD_LANGUAGE}-{NEW_LANGUAGE}.{NEW_LANGUAGE} corpora/parallel/{corpus}/{corpus}.{OLD_LANGUAGE}-{NEW_LANGUAGE}.{NEW_LANGUAGE}')\n",
    "                        return\n",
    "                    # check if the zip file was empty\n",
    "                    if os.path.getsize(f'corpora/parallel/{corpus}/{corpus}.{OLD_LANGUAGE}-{NEW_LANGUAGE}.txt.zip') == 0:\n",
    "                        # try with the language codes swapped\n",
    "                        os.system(f'rm corpora/parallel/{corpus}/{corpus}.{OLD_LANGUAGE}-{NEW_LANGUAGE}.txt.zip')\n",
    "                        if not os.path.exists(f'corpora/parallel/{corpus}/{corpus}.{NEW_LANGUAGE}-{OLD_LANGUAGE}.txt.zip'):\n",
    "                            os.system(f'wget -O corpora/parallel/{corpus}/{corpus}.{NEW_LANGUAGE}-{OLD_LANGUAGE}.txt.zip https://opus.nlpl.eu/download.php?f={corpus_folder}/{corpus_version}/moses/{NEW_LANGUAGE}-{OLD_LANGUAGE}.txt.zip')\n",
    "                        os.system(f'cd corpora/parallel/{corpus} && unzip {corpus}.{NEW_LANGUAGE}-{OLD_LANGUAGE}.txt.zip')\n",
    "                    if not os.path.exists(f'corpora/parallel/{corpus}/{corpus}.{NEW_LANGUAGE}-{OLD_LANGUAGE}.{OLD_LANGUAGE}') or not os.path.exists(f'corpora/parallel/{corpus}/{corpus}.{NEW_LANGUAGE}-{OLD_LANGUAGE}.{NEW_LANGUAGE}'):\n",
    "                        # check if the files are there but with corpus_folder as their prefix\n",
    "                        if os.path.exists(f'corpora/parallel/{corpus}/{corpus_folder}.{OLD_LANGUAGE}-{NEW_LANGUAGE}.{OLD_LANGUAGE}') and os.path.exists(f'corpora/parallel/{corpus}/{corpus_folder}.{OLD_LANGUAGE}-{NEW_LANGUAGE}.{NEW_LANGUAGE}'):\n",
    "                            os.system(f'mv corpora/parallel/{corpus}/{corpus_folder}.{OLD_LANGUAGE}-{NEW_LANGUAGE}.{OLD_LANGUAGE} corpora/parallel/{corpus}/{corpus}.{OLD_LANGUAGE}-{NEW_LANGUAGE}.{OLD_LANGUAGE}')\n",
    "                            os.system(f'mv corpora/parallel/{corpus}/{corpus_folder}.{OLD_LANGUAGE}-{NEW_LANGUAGE}.{NEW_LANGUAGE} corpora/parallel/{corpus}/{corpus}.{OLD_LANGUAGE}-{NEW_LANGUAGE}.{NEW_LANGUAGE}')\n",
    "                        else:\n",
    "                            print(f'{corpus}: could not download data')\n",
    "                            raise Exception(f'Corpus not found; download files corpora/parallel/{corpus}/{corpus}.{OLD_LANGUAGE}-{NEW_LANGUAGE}.{OLD_LANGUAGE} and corpora/parallel/{corpus}/{corpus}.{OLD_LANGUAGE}-{NEW_LANGUAGE}.{NEW_LANGUAGE}')\n",
    "                    else:\n",
    "                        os.system(f'mv corpora/parallel/{corpus}/{corpus}.{NEW_LANGUAGE}-{OLD_LANGUAGE}.{OLD_LANGUAGE} corpora/parallel/{corpus}/{corpus}.{OLD_LANGUAGE}-{NEW_LANGUAGE}.{OLD_LANGUAGE}')\n",
    "                        os.system(f'mv corpora/parallel/{corpus}/{corpus}.{NEW_LANGUAGE}-{OLD_LANGUAGE}.{NEW_LANGUAGE} corpora/parallel/{corpus}/{corpus}.{OLD_LANGUAGE}-{NEW_LANGUAGE}.{NEW_LANGUAGE}')\n",
    "\n",
    "        for corpus in CORPUS_LIST:\n",
    "            if corpus == 'NLLB':\n",
    "                download_corpus_from_nlpl_eu('NLLB')\n",
    "            if corpus == 'OpenSubtitles2018':\n",
    "                download_corpus_from_nlpl_eu('OpenSubtitles2018','OpenSubtitles','v2018')\n",
    "        \n",
    "        for corpus in CORPUS_LIST:\n",
    "            if not os.path.exists(f'corpora/parallel/{corpus}/{corpus}.{OLD_LANGUAGE}-{NEW_LANGUAGE}.{OLD_LANGUAGE}') or not os.path.exists(f'corpora/parallel/{corpus}/{corpus}.{OLD_LANGUAGE}-{NEW_LANGUAGE}.{NEW_LANGUAGE}'):\n",
    "                print(f'{corpus}: corpus not found')\n",
    "                raise Exception(f'Corpus not found; download files corpora/parallel/{corpus}/{corpus}.{OLD_LANGUAGE}-{NEW_LANGUAGE}.{OLD_LANGUAGE} and corpora/parallel/{corpus}/{corpus}.{OLD_LANGUAGE}-{NEW_LANGUAGE}.{NEW_LANGUAGE}')\n",
    "\n",
    "        #\n",
    "        # step 2: preprocess the parallel corpus\n",
    "        #\n",
    "        \n",
    "        for corpus in CORPUS_LIST:\n",
    "            if os.path.exists(f'corpora/parallel/{corpus}/{corpus}.{OLD_LANGUAGE}-{NEW_LANGUAGE}.{OLD_LANGUAGE}.{OLD_TOKENIZER_FRIENDLY_NAME}.txt'):\n",
    "                print(f'{corpus}: data already preprocessed for tokenizer {OLD_TOKENIZER}')\n",
    "            else:\n",
    "                print(f'{corpus}: preprocessing data for tokenizer {OLD_TOKENIZER}...')\n",
    "                with open(f'corpora/parallel/{corpus}/{corpus}.{OLD_LANGUAGE}-{NEW_LANGUAGE}.{OLD_LANGUAGE}.{OLD_TOKENIZER_FRIENDLY_NAME}.txt','w') as f:\n",
    "                    with open(f'corpora/parallel/{corpus}/{corpus}.{OLD_LANGUAGE}-{NEW_LANGUAGE}.{OLD_LANGUAGE}') as g:\n",
    "                        for line in tqdm(g):\n",
    "                            f.write(' '.join(old_tokenizer.tokenize(line.strip()))+'\\n')\n",
    "                            \n",
    "            if os.path.exists(f'corpora/parallel/{corpus}/{corpus}.{OLD_LANGUAGE}-{NEW_LANGUAGE}.{NEW_LANGUAGE}.{NEW_TOKENIZER_FRIENDLY_NAME}.txt'):\n",
    "                print(f'{corpus}: data already preprocessed for tokenizer {NEW_TOKENIZER}')\n",
    "            else:\n",
    "                print(f'{corpus}: preprocessing data for tokenizer {NEW_TOKENIZER}...')\n",
    "                with open(f'corpora/parallel/{corpus}/{corpus}.{OLD_LANGUAGE}-{NEW_LANGUAGE}.{NEW_LANGUAGE}.{NEW_TOKENIZER_FRIENDLY_NAME}.txt','w') as f:\n",
    "                    with open(f'corpora/parallel/{corpus}/{corpus}.{OLD_LANGUAGE}-{NEW_LANGUAGE}.{NEW_LANGUAGE}') as g:\n",
    "                        for line in tqdm(g):\n",
    "                            f.write(' '.join(new_tokenizer.tokenize(line.strip()))+'\\n')\n",
    "\n",
    "    #\n",
    "    # step 3: combine the two tokenized corpora into a single parallel corpus in the format expected by fast_align (a1 a2 a3 ||| b1 b2 b3)\n",
    "    #\n",
    "\n",
    "    if os.path.exists(f'alignments/{CORPUS_LIST_STR}.{OLD_LANGUAGE}-{NEW_LANGUAGE}.{OLD_TOKENIZER_FRIENDLY_NAME}-{NEW_TOKENIZER_FRIENDLY_NAME}-{ALIGNMENT_UNIT}.moses'):\n",
    "        print(f'data already preprocessed for fast_align')\n",
    "    else:\n",
    "        print(f'preprocessing data for fast_align...')\n",
    "        with open(f'alignments/{CORPUS_LIST_STR}.{OLD_LANGUAGE}-{NEW_LANGUAGE}.{OLD_TOKENIZER_FRIENDLY_NAME}-{NEW_TOKENIZER_FRIENDLY_NAME}-{ALIGNMENT_UNIT}.moses','w') as f:\n",
    "            for corpus in CORPUS_LIST:\n",
    "                with open(f'corpora/parallel/{corpus}/{corpus}.{OLD_LANGUAGE}-{NEW_LANGUAGE}.{OLD_LANGUAGE}.{OLD_TOKENIZER_FRIENDLY_NAME}.txt') as g:\n",
    "                    with open(f'corpora/parallel/{corpus}/{corpus}.{OLD_LANGUAGE}-{NEW_LANGUAGE}.{NEW_LANGUAGE}.{NEW_TOKENIZER_FRIENDLY_NAME}.txt') as h:\n",
    "                        for line1,line2 in tqdm(zip(g,h)):\n",
    "                            if ALIGNMENT_UNIT == 'WORDS':\n",
    "                                # merging tokens from word units, for a better alignment\n",
    "                                line1 = re.sub(r'(?!'+OLD_TOKENIZER_1ST_PREFIX+r')(\\p{L})[ ](?!'+OLD_TOKENIZER_1ST_PREFIX+r')(?='+OLD_TOKENIZER_2ND_PREFIX+r'\\p{L})',r'\\1—',line1)\n",
    "                                line2 = re.sub(r'(?!'+NEW_TOKENIZER_1ST_PREFIX+r')(\\p{L})[ ](?!'+NEW_TOKENIZER_1ST_PREFIX+r')(?='+NEW_TOKENIZER_2ND_PREFIX+r'\\p{L})',r'\\1—',line2)\n",
    "                            f.write(line1.strip()+' ||| '+line2.strip()+'\\n')\n",
    "    #\n",
    "    # step 4: download fast_align if not already downloaded\n",
    "    #\n",
    "    if not os.path.exists('fast_align/build/fast_align'):\n",
    "        print(f'downloading fast_align...')\n",
    "        os.system('apt-get install -y libgoogle-perftools-dev libsparsehash-dev')\n",
    "        os.system('git clone https://github.com/FremyCompany/fast_align.git')\n",
    "        os.system('mkdir fast_align/build')\n",
    "        os.system('cd fast_align/build && cmake .. && make')\n",
    "\n",
    "    #\n",
    "    # step 5: using fast_align to align the two tokenized corpora\n",
    "    #\n",
    "    if os.path.exists(f'alignments/{CORPUS_LIST_STR}.{OLD_LANGUAGE}-{NEW_LANGUAGE}.{OLD_TOKENIZER_FRIENDLY_NAME}-{NEW_TOKENIZER_FRIENDLY_NAME}-{ALIGNMENT_UNIT}.fast_align.tsv'):\n",
    "        print(f'data already aligned')\n",
    "    else:\n",
    "        os.system(f\"./fast_align/build/fast_align -I 7 -p alignments/{CORPUS_LIST_STR}.{OLD_LANGUAGE}-{NEW_LANGUAGE}.{OLD_TOKENIZER_FRIENDLY_NAME}-{NEW_TOKENIZER_FRIENDLY_NAME}-{ALIGNMENT_UNIT}.fast_align.tsv -i alignments/{CORPUS_LIST_STR}.{OLD_LANGUAGE}-{NEW_LANGUAGE}.{OLD_TOKENIZER_FRIENDLY_NAME}-{NEW_TOKENIZER_FRIENDLY_NAME}-{ALIGNMENT_UNIT}.moses > /dev/null\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the alignments to create a weighted dictionary of possible translations for each token\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "tokenized_possible_translations = defaultdict(lambda: defaultdict(int))\n",
    "untokenized_possible_translations = defaultdict(lambda: defaultdict(int)) # only filled when ALIGNMENT_UNIT is 'WORDS', and for diagnostics purposes only\n",
    "\n",
    "def add_token_pair(count, new_token, old_token):\n",
    "    tokenized_possible_translations[new_token][old_token] += count\n",
    "\n",
    "def add_word_pair(count, new_word, old_word, all_to_all_mapping=False):\n",
    "    # tokenize the words\n",
    "    # (recall that we use the long hyphen to replace spaces inside words, to merge the tokens again)\n",
    "    old_word_tokenized = old_word.split('—')\n",
    "    new_word_tokenized = new_word.split('—')\n",
    "\n",
    "    # if the token list dont have the same length, compute the smallest common multiple of their lengths\n",
    "    if all_to_all_mapping:\n",
    "        count_dilution = len(old_word_tokenized)\n",
    "        old_word_tokenized = np.tile(old_word_tokenized, len(new_word_tokenized))\n",
    "        new_word_tokenized = np.repeat(new_word_tokenized, count_dilution)\n",
    "    elif len(old_word_tokenized) != len(new_word_tokenized):\n",
    "        gcd = math.gcd(len(old_word_tokenized), len(new_word_tokenized))\n",
    "        count_dilution = len(old_word_tokenized) // gcd\n",
    "        old_word_tokenized = np.repeat(old_word_tokenized, len(new_word_tokenized) // gcd)\n",
    "        new_word_tokenized = np.repeat(new_word_tokenized, count_dilution)\n",
    "    else:\n",
    "        gcd = 1\n",
    "        count_dilution = 1\n",
    "\n",
    "    # perform this operation for each token pair in the word\n",
    "    for token_old, token_new in zip(old_word_tokenized, new_word_tokenized):\n",
    "        # add the translation to the dictionary\n",
    "        tokenized_possible_translations[token_new][token_old] += max(1, count // count_dilution)\n",
    "\n",
    "total_alignments = 0\n",
    "with open(f'alignments/{CORPUS_LIST_STR}.{OLD_LANGUAGE}-{NEW_LANGUAGE}.{OLD_TOKENIZER_FRIENDLY_NAME}-{NEW_TOKENIZER_FRIENDLY_NAME}-{ALIGNMENT_UNIT}.fast_align.tsv') as f:\n",
    "    for line in f: total_alignments += 1\n",
    "\n",
    "with open(f'alignments/{CORPUS_LIST_STR}.{OLD_LANGUAGE}-{NEW_LANGUAGE}.{OLD_TOKENIZER_FRIENDLY_NAME}-{NEW_TOKENIZER_FRIENDLY_NAME}-{ALIGNMENT_UNIT}.fast_align.tsv') as f:\n",
    "    for line in tqdm(f, total=total_alignments):\n",
    "        # remove the newline character\n",
    "        line = line.rstrip('\\n')\n",
    "        # skip empty lines\n",
    "        if line == '': continue\n",
    "        # split the line on the tab character\n",
    "        old_word, new_word, log_prob, count = line.split('\\t')\n",
    "        # reject <eps> mappings\n",
    "        if old_word == '<eps>': continue\n",
    "        if new_word == '<eps>': continue\n",
    "        # convert the count to an integer\n",
    "        count = int(float(count))\n",
    "        # skip pairs that happened rarely (likely noise)\n",
    "        if count < MIN_COUNT_REQUIRED_FOR_CONSIDERATION: continue\n",
    "        # add the token pair to the token dictionary\n",
    "        if (ALIGNMENT_UNIT != 'WORDS') or ((new_word in new_tokenizer_vocab) and (old_word in old_tokenizer_vocab)):\n",
    "            add_token_pair(count, new_word, old_word)\n",
    "        else:\n",
    "            half_count = max(1, count // 2)\n",
    "            add_word_pair(half_count, new_word, old_word, all_to_all_mapping=True)\n",
    "            add_word_pair(half_count, new_word, old_word, all_to_all_mapping=False)\n",
    "        # add the word translation to the dictionary (for diagnostics purposes only)\n",
    "        untokenized_possible_translations[new_word][old_word] += count\n",
    "\n",
    "# add a mapping for all numbers\n",
    "for i in range(9999):\n",
    "    str_i = str(i)\n",
    "    if str_i in new_tokenizer_vocab:\n",
    "        add_token_pair(1, str_i, str_i if str_i in old_tokenizer_vocab else old_tokenizer.tokenize(str_i)[0])\n",
    "    if len(new_tokenizer.tokenize(str_i)) == 1:\n",
    "        add_token_pair(1, new_tokenizer.tokenize(str_i)[0], old_tokenizer.tokenize(str_i)[0])\n",
    "    if len(new_tokenizer.tokenize(' ' + str_i)) == 1:\n",
    "        add_token_pair(1, new_tokenizer.tokenize(' ' + str_i)[0], old_tokenizer.tokenize(' ' + str_i)[0])\n",
    "for i in range(99):\n",
    "    str_i = '0' + str(i)\n",
    "    if str_i in new_tokenizer_vocab:\n",
    "        add_token_pair(1, str_i, str_i if str_i in old_tokenizer_vocab else old_tokenizer.tokenize(str_i)[0])\n",
    "    if len(new_tokenizer.tokenize(str_i)) == 1:\n",
    "        add_token_pair(1, new_tokenizer.tokenize(str_i)[0], old_tokenizer.tokenize(str_i)[0])\n",
    "    if len(new_tokenizer.tokenize(' ' + str_i)) == 1:\n",
    "        add_token_pair(1, new_tokenizer.tokenize(' ' + str_i)[0], old_tokenizer.tokenize(' ' + str_i)[0])\n",
    "\n",
    "# add a mapping for all punctuation (and words that exist in both models)\n",
    "for token in new_tokenizer_vocab:\n",
    "    ## skip if any token char is a letter or digit\n",
    "    #if any(c.isalnum() for c in token): continue\n",
    "    # replace the start symbol of the new model with the one of the old model\n",
    "    if NEW_TOKENIZER_1ST_PREFIX != '' or OLD_TOKENIZER_1ST_PREFIX != '':\n",
    "        token2 = token.replace(NEW_TOKENIZER_1ST_PREFIX, OLD_TOKENIZER_1ST_PREFIX)\n",
    "    # replace the continuation symbol of the new model with the one of the old model\n",
    "    if NEW_TOKENIZER_2ND_PREFIX != '' or OLD_TOKENIZER_2ND_PREFIX != '':\n",
    "        token2 = token2.replace(NEW_TOKENIZER_2ND_PREFIX, OLD_TOKENIZER_2ND_PREFIX)\n",
    "    # skip if token is not in the old model\n",
    "    if token2 not in old_tokenizer_vocab: continue\n",
    "    # add the mapping\n",
    "    tokenized_possible_translations[token][token2] += 1\n",
    "\n",
    "def or_old_unk_token(token, fallback_token=None):\n",
    "    if (token != None) and (token in old_tokenizer_vocab): return token\n",
    "    if (fallback_token != None) and (fallback_token in old_tokenizer_vocab): return fallback_token\n",
    "    return old_tokenizer.unk_token\n",
    "\n",
    "# add a mapping for special tokens (i.e. pad, unk, bos, eos, sep, cls, mask)\n",
    "very_large_number = 1_000_000_000\n",
    "if new_tokenizer.pad_token != None: add_token_pair(very_large_number, new_tokenizer.pad_token, or_old_unk_token(old_tokenizer.pad_token))\n",
    "if new_tokenizer.unk_token != None: add_token_pair(very_large_number, new_tokenizer.unk_token, or_old_unk_token(old_tokenizer.unk_token))\n",
    "if new_tokenizer.bos_token != None: add_token_pair(very_large_number, new_tokenizer.bos_token, or_old_unk_token(old_tokenizer.bos_token, old_tokenizer.cls_token))\n",
    "if new_tokenizer.eos_token != None: add_token_pair(very_large_number, new_tokenizer.eos_token, or_old_unk_token(old_tokenizer.eos_token, old_tokenizer.sep_token))\n",
    "if new_tokenizer.cls_token != None: add_token_pair(very_large_number, new_tokenizer.cls_token, or_old_unk_token(old_tokenizer.cls_token, old_tokenizer.bos_token))\n",
    "if new_tokenizer.sep_token != None: add_token_pair(very_large_number, new_tokenizer.sep_token, or_old_unk_token(old_tokenizer.sep_token, old_tokenizer.eos_token))\n",
    "if new_tokenizer.mask_token != None: add_token_pair(very_large_number, new_tokenizer.mask_token, or_old_unk_token(old_tokenizer.mask_token, old_tokenizer.pad_token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coefficients_for_token(new_token):\n",
    "    # get the possible translations for this token\n",
    "    possible_translations = tokenized_possible_translations[new_token]\n",
    "    # get the total count of all translations\n",
    "    total_count = sum(possible_translations.values())\n",
    "    # compute the probability of each translation\n",
    "    probabilities = {old_token: count / total_count for old_token, count in possible_translations.items()}\n",
    "    # sort the translations by probability\n",
    "    probabilities = sorted(probabilities.items(), key=lambda x: x[1], reverse=True)\n",
    "    # return the probabilities\n",
    "    return probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print some pairs to see if it worked\n",
    "def print_get_coefficients_for_token(token):\n",
    "    print(f'{token}: {get_coefficients_for_token(token)}')\n",
    "\n",
    "print_get_coefficients_for_token('Ġromige')\n",
    "print_get_coefficients_for_token('ĠRomige')\n",
    "\n",
    "print_get_coefficients_for_token('Ġkleine')\n",
    "print_get_coefficients_for_token('ĠKleine')\n",
    "\n",
    "print_get_coefficients_for_token('zee')\n",
    "print_get_coefficients_for_token('Ġzee')\n",
    "\n",
    "print_get_coefficients_for_token('ster')\n",
    "print_get_coefficients_for_token('Ġster')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check how many tokens have a translation, compared to the total number of tokens\n",
    "print(f'Number of tokens with a translation: {len(tokenized_possible_translations)}')\n",
    "print(f'Number of tokens without a translation: {len(new_tokenizer)}')\n",
    "print(f'Percentage of tokens with a translation: {int(len(tokenized_possible_translations) / len(new_tokenizer) * 1000)/10}%')\n",
    "\n",
    "tokenized_possible_translations_step2 = tokenized_possible_translations.copy()\n",
    "\n",
    "## for tokens with start with a space, double the weight of their translations that also start with a space\n",
    "#for token, translations in tokenized_possible_translations_step2.items():\n",
    "#    if token.startswith(NEW_TOKENIZER_1ST_PREFIX):\n",
    "#        for translation, count in translations.items():\n",
    "#            if translation.startswith(OLD_TOKENIZER_1ST_PREFIX):\n",
    "#                translations[translation] *= 2\n",
    "\n",
    "# print the first 100 tokens that have no translation\n",
    "tmp_count = 0\n",
    "for i, token in enumerate(new_tokenizer.get_vocab()):\n",
    "    #if tmp_count >= 100: break\n",
    "    if token not in tokenized_possible_translations:\n",
    "        tmp_count += 1\n",
    "        # provide a list of tokens which start with the same characters\n",
    "        similar_tokens = [token2 for token2 in new_tokenizer.get_vocab() if token2.startswith(token) and (token2 in tokenized_possible_translations)]\n",
    "        ## find the tokens which are the start of this token\n",
    "        #start_subset_tokens = [token2 for token2 in tokenized_possible_translations if token.startswith(token2) and (token2 in tokenized_possible_translations)]\n",
    "        #start_subset_tokens.sort(key=lambda x: len(x), reverse=True)\n",
    "        ## find the tokens which are the end of this token\n",
    "        #end_subset_tokens = [token2 for token2 in tokenized_possible_translations if token.endswith(token2) and (token2 in tokenized_possible_translations)]\n",
    "        #end_subset_tokens.sort(key=lambda x: len(x), reverse=True)\n",
    "        # find the tokens which are the middle of this token\n",
    "        middle_subset_tokens = [token2 for token2 in tokenized_possible_translations if (token2 in token) and (token2 in tokenized_possible_translations)]\n",
    "        middle_subset_tokens.sort(key=lambda x: len(x), reverse=True)\n",
    "        # remove the tokens which are included in another previous token of the list\n",
    "        #start_subset_tokens = [token2 for i, token2 in enumerate(start_subset_tokens) if (i == 0) or not any([token2 in token3 for token3 in start_subset_tokens[0:i]])]\n",
    "        #end_subset_tokens = [token2 for i, token2 in enumerate(end_subset_tokens) if (i == 0) or not any([token2 in token3 for token3 in end_subset_tokens[0:i]])]\n",
    "        middle_subset_tokens = [token2 for i, token2 in enumerate(middle_subset_tokens) if (i == 0) or not any([token2 in token3 for token3 in middle_subset_tokens[0:i]])]\n",
    "        # sort the middle tokens by position in the token\n",
    "        middle_subset_tokens.sort(key=lambda x: token.index(x))\n",
    "        # print the token, the similar tokens, and the start, end, and middle subset tokens\n",
    "        if tmp_count <= 100: print(token, similar_tokens, middle_subset_tokens) #start_subset_tokens[0:3], end_subset_tokens[0:3], middle_subset_tokens[0:3])\n",
    "        # add the token to the updated dictionary\n",
    "        if len(similar_tokens) == 0 and len(middle_subset_tokens) == 0: continue\n",
    "        tokenized_possible_translations_step2[token] = defaultdict(int)\n",
    "        for token2 in similar_tokens + middle_subset_tokens:\n",
    "            # add all their translation to the dictionary, normalizing to a total count of 1000 for each token2 (2000 if the token starts with a space)\n",
    "            count_for_token2 = sum(tokenized_possible_translations[token2].values())\n",
    "            if count_for_token2 > 0:\n",
    "                for translation_of_token2 in tokenized_possible_translations[token2]:\n",
    "                    weight = 2000 if translation_of_token2.startswith('▁') else 1000\n",
    "                    tokenized_possible_translations_step2[token][translation_of_token2] += max(1, (weight * tokenized_possible_translations[token2][translation_of_token2]) // count_for_token2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check how many tokens have a translation, compared to the total number of tokens\n",
    "print(f'Number of tokens with a translation: {len(tokenized_possible_translations_step2)}')\n",
    "print(f'Number of tokens without a translation: {len(new_tokenizer)}')\n",
    "print(f'Percentage of tokens with a translation: {int(len(tokenized_possible_translations_step2) / len(new_tokenizer) * 1000)/10}%')\n",
    "\n",
    "# print the first 100 tokens that have no translation\n",
    "tmp_count = 0\n",
    "for i, token in enumerate(new_tokenizer.get_vocab()):\n",
    "    #if tmp_count >= 100: break\n",
    "    if token not in tokenized_possible_translations_step2:\n",
    "        tmp_count += 1\n",
    "        print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def get_coefficients_for_token_step2(new_token):\n",
    "    # check for unmapped tokens\n",
    "    if new_token not in tokenized_possible_translations_step2: return [(old_tokenizer.unk_token, 1.0)]\n",
    "    # get the possible translations for this token\n",
    "    possible_translations = tokenized_possible_translations_step2[new_token]\n",
    "    # get the total count of all translations\n",
    "    total_count = sum(possible_translations.values())\n",
    "    # check for unmapped tokens by count\n",
    "    if total_count <= 0: return [(old_tokenizer.unk_token, 1.0)]\n",
    "    # compute the probability of each translation\n",
    "    probabilities = {old_token: count / total_count for old_token, count in possible_translations.items()}\n",
    "    # sort the translations by probability\n",
    "    probabilities = sorted(probabilities.items(), key=lambda x: x[1], reverse=True)\n",
    "    # return the probabilities\n",
    "    return probabilities\n",
    "\n",
    "# convert the dictionary to a list of sorted lists, and save it to a json file\n",
    "final_list = []\n",
    "for token_i in range(len(new_tokenizer.vocab)):\n",
    "    token = new_tokenizer.convert_ids_to_tokens(token_i)\n",
    "    coefficients = get_coefficients_for_token_step2(token)\n",
    "    final_list.append((token, coefficients))\n",
    "json.dump(final_list, open(f'alignments/{CORPUS_LIST_STR}.{OLD_LANGUAGE}-{NEW_LANGUAGE}.{OLD_TOKENIZER_FRIENDLY_NAME}-{NEW_TOKENIZER_FRIENDLY_NAME}-{ALIGNMENT_UNIT}-{MIN_COUNT_REQUIRED_FOR_CONSIDERATION}.token_mapping.json', 'w'), indent='\\t')\n",
    "\n",
    "# created a more compact version of the json file\n",
    "final_list_compact = {token: \",  \".join([f'{old_token} {int(100*probability)}%' for old_token, probability in coefficients]) for token, coefficients in final_list}\n",
    "#json.dump(final_list_compact, open(f'alignments/{CORPUS_LIST_STR}.{OLD_LANGUAGE}-{NEW_LANGUAGE}.{OLD_TOKENIZER_FRIENDLY_NAME}-{NEW_TOKENIZER_FRIENDLY_NAME}-{ALIGNMENT_UNIT}-{MIN_COUNT_REQUIRED_FOR_CONSIDERATION}.token_mapping.json.compact.json', 'w'), indent='\\t')\n",
    "with open(f'alignments/{CORPUS_LIST_STR}.{OLD_LANGUAGE}-{NEW_LANGUAGE}.{OLD_TOKENIZER_FRIENDLY_NAME}-{NEW_TOKENIZER_FRIENDLY_NAME}-{ALIGNMENT_UNIT}-{MIN_COUNT_REQUIRED_FOR_CONSIDERATION}.token_mapping.json.compact.txt', 'w') as f:\n",
    "    for key, value in final_list_compact.items():\n",
    "        f.write(f'{key}\\t{value}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
